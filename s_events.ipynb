{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Events scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def available(s):\n",
    "    if s.find('Apply for Tickets')>=0:\n",
    "        a = 'Available'\n",
    "    elif s.find('Event is full') >= 0:\n",
    "        a = 'Sold out'\n",
    "    else: a = s\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "def cleandate(s):\n",
    "    return re.sub(r'(\\d)(st|nd|rd|th)', r'\\1', s)\n",
    "#d = \n",
    "#d = datetime.strptime('Tuesday, 5th', '%A, %d')\n",
    "#d = datetime.strptime(cleandate('Friday, December 1st'), '%A, %B %d').strftime('%d/%m/2017')\n",
    "#d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "domain = 'https://www.sofarsounds.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlalchemy as sqa\n",
    "import pandas as pd\n",
    "creds = {\n",
    "    'user': 'ucoq7h2sabk90n',\n",
    "    'password': 'p9707226g0i18s86582v9or2081',\n",
    "    'hostname': 'ec2-52-30-189-58.eu-west-1.compute.amazonaws.com',\n",
    "    'db': 'd5srqo93idqsph'\n",
    "}\n",
    "\n",
    "engine = sqa.create_engine('postgresql://{user}:{password}@{hostname}:5432/{db}'.format(**creds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get cities list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "select cached_slug\n",
    "from public.cities;\n",
    "\"\"\"\n",
    "dfo = pd.read_sql_query(sql, engine)\n",
    "#cities = ['oslo','madrid','london','liverpool','manchester','glasgow','leeds','nyc','la','chicago','san-francisco','toronto','seattle','boston','austin','dallas-fort-worth','berlin','nuremberg']\n",
    "cities = dfo['cached_slug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryan/anaconda2/envs/ryan/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /Users/ryan/anaconda2/envs/ryan/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# build a list of all events for each city\n",
    "events = []\n",
    "city_with_no_gigs = []\n",
    "for city in cities:\n",
    "    # go to city events page\n",
    "    page = urllib2.urlopen(domain+'/'+city+'/events')\n",
    "    \n",
    "    # parse and find the events list\n",
    "    soup = BeautifulSoup(page)\n",
    "    event_list = soup.find(\"div\",class_='events-row')\n",
    "\n",
    "    # build a list of the events\n",
    "    try:\n",
    "        all_links=event_list.find_all(\"a\")\n",
    "        for link in all_links:\n",
    "            if link.get(\"href\").find('/events/') > 0: \n",
    "                events.append(link.get(\"href\"))\n",
    "    except: \n",
    "        city_with_no_gigs.append(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b8373c9c1a4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclosest_stations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mavl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"shortcut-container\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mavailability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mapply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavailable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "locations = []\n",
    "dates = []\n",
    "closest_stations = []\n",
    "arrival_times = []\n",
    "availability = []\n",
    "apply = []\n",
    "event_id = []\n",
    "\n",
    "\n",
    "for event in events:\n",
    "\n",
    "    if event.find('/events/') < 0: continue\n",
    "\n",
    "    #print(domain+event)\n",
    "    event_page = urllib2.urlopen(domain+event)\n",
    "    soup = BeautifulSoup(event_page)\n",
    "\n",
    "    event_id.append(event)\n",
    "\n",
    "    try: locations.append(soup.find(\"span\",class_='event-rough-location').text)\n",
    "    except: locations.append('none')\n",
    "\n",
    "    try: dates.append(soup.find(\"span\",class_='event-date').text)\n",
    "    except: dates.append('none')\n",
    "    \n",
    "    try: arrival_times.append(soup.find(\"span\",class_='event-arrival-time').text)\n",
    "    except: arrival_times.append('none')\n",
    "\n",
    "    try: closest_stations.append(soup.find(\"span\",class_='event-closest-station').text)\n",
    "    except: closest_stations.append('none')\n",
    "\n",
    "    avl = soup.find(\"div\",{\"class\":\"shortcut-container\"}).text.strip()\n",
    "    availability.append(avl)\n",
    "    apply.append(available(avl))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(event_id,columns=['Event'])\n",
    "df['Location']=locations\n",
    "df['Dates']=dates\n",
    "df['Closest_Station']=closest_stations\n",
    "df['Arrival_time']=arrival_times\n",
    "#df['Availability']=availability\n",
    "df['Apply']=apply\n",
    "df['Scrape_Time']=datetime.datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = datetime.datetime.today().strftime('%Y%m%d_%H%M') + '_sofar_scrape.csv'\n",
    "df.to_csv('/Users/ryan/Google Drive/Personal/Sofar/'+filename,index=False,encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
